{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, Bidirectional, Embedding, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'id': 'KG0OUA', 'data': 'Good morning', 'message_order': 2, 'comments': ['']}</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'id': 'L9DC9H', 'data': 'Location', 'message_order': 5, 'comments': ['']}</td>\n",
       "      <td>whoAreYou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'id': 'ZQR6R5', 'data': 'hi', 'message_order': 5, 'comments': ['']}</td>\n",
       "      <td>whoAreYou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'id': 'RH0M4E', 'data': 'Hi', 'message_order': 4, 'comments': ['']}</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'id': 'WLVX8I', 'data': 'Hello', 'message_order': 1, 'comments': ['']}</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                             data  \\\n",
       "0  {'id': 'KG0OUA', 'data': 'Good morning', 'message_order': 2, 'comments': ['']}   \n",
       "1  {'id': 'L9DC9H', 'data': 'Location', 'message_order': 5, 'comments': ['']}       \n",
       "2  {'id': 'ZQR6R5', 'data': 'hi', 'message_order': 5, 'comments': ['']}             \n",
       "3  {'id': 'RH0M4E', 'data': 'Hi', 'message_order': 4, 'comments': ['']}             \n",
       "4  {'id': 'WLVX8I', 'data': 'Hello', 'message_order': 1, 'comments': ['']}          \n",
       "\n",
       "       label  \n",
       "0  location   \n",
       "1  whoAreYou  \n",
       "2  whoAreYou  \n",
       "3  greeting   \n",
       "4  greeting   "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('TakeHome_task_data.csv')\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>message_order</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'Good morning'</td>\n",
       "      <td>2</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'Location'</td>\n",
       "      <td>5</td>\n",
       "      <td>whoAreYou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'hi'</td>\n",
       "      <td>5</td>\n",
       "      <td>whoAreYou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'Hi'</td>\n",
       "      <td>4</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'Hello'</td>\n",
       "      <td>1</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              text  message_order      label\n",
       "0   'Good morning'  2              location \n",
       "1   'Location'      5              whoAreYou\n",
       "2   'hi'            5              whoAreYou\n",
       "3   'Hi'            4              greeting \n",
       "4   'Hello'         1              greeting "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#obtaining the important parts of the data from the csv file i.e 'message', 'message_order', 'intent label'\n",
    "\n",
    "ndf = df['data'].str.split(',')\n",
    "\n",
    "ndata = list()\n",
    "for i in range(ndf.size):\n",
    "    ndata.append(ndf[i][1])\n",
    "    \n",
    "final_data = list()\n",
    "for i in range(len(ndata)):\n",
    "    final_data.append(ndata[i].split(':')[1])\n",
    "    \n",
    "m_order = list()\n",
    "for i in range(ndf.size):\n",
    "    if i in (75, 247, 458, 464, 631, 1269, 1459, 1601, 1651, 1803, 1918): #a bit crude but works for now\n",
    "        m_order.append(ndf[i][3])\n",
    "    else:\n",
    "        m_order.append(ndf[i][2])\n",
    "    \n",
    "final_m_order = list()\n",
    "for i in range(len(m_order)):\n",
    "    final_m_order.append(m_order[i].split(':')[1])\n",
    "\n",
    "    \n",
    "data = pd.DataFrame(final_data, columns = ['text']).assign(message_order = final_m_order)\n",
    "data = data.assign(label = df.label)\n",
    "data.text = data.text.apply(str)\n",
    "data.message_order = pd.to_numeric(data.message_order)\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent = data.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I want to look for any relations we can find using the message_order field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dontMeetRequirements', 'whoAreYou', 'greeting', 'location', 'notInterested']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_intents = list(set(data.label))\n",
    "unique_intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.replace({'location': 1,'whoAreYou': 2, 'greeting' : 3, 'dontMeetRequirements' : 4, 'notInterested' : 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output pre_processed csv file\n",
    "data.to_csv(r'pre_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualizing distribution of intent classification over message_orders (the dataset has fewer examples with class = 4,5)\n",
    "\n",
    "x = np.array(data.message_order)\n",
    "y = np.array(data.label)\n",
    "\n",
    "plt.xlabel('Message Order')\n",
    "plt.ylabel('Intent Class')\n",
    "\n",
    "plt.scatter(x,y, alpha = 0.007)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of class 1 = 120\n",
      "Examples of class 2 = 627\n",
      "Examples of class 3 = 619\n",
      "Examples of class 4 = 160\n",
      "Examples of class 5 = 474\n"
     ]
    }
   ],
   "source": [
    "message = np.array(data.text)\n",
    "message_order = np.array(data.message_order)\n",
    "labels = np.array(data.label)\n",
    "\n",
    "for i in range(5):\n",
    "    print('Examples of class ' + str(i+1) + ' =', np.sum(labels == i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#To see if we can immediately spot any relation between the 'message' and 'message_order', let us vary\n",
    "#one of them while holding the other constant and see the effect on intent class distribution\n",
    "\n",
    "#Let us try to isolate the message 'hi' (600+ training examples) and see if we can find any relation between the\n",
    "#assigned intent and message_order by keeping the text input from user constant i.e 'hi'\n",
    "\n",
    "#if message_order is the order in which the message was sent by the user, then we would expect a lot of message_order = 0 \n",
    "# 'hi' to be greetings.\n",
    "\n",
    "#getting indices for messages that start with 'hi'\n",
    "mask = []\n",
    "for n,m in enumerate(message):\n",
    "    m = m.lower()\n",
    "    if m.startswith('h', 2):\n",
    "        mask.append(n)\n",
    "\n",
    "#now getting the intent class for various message_orders of the similar messages ('hi')\n",
    "a = labels[mask]\n",
    "for i in range(6):\n",
    "    x = a[message_order[mask] == i]\n",
    "    plt.subplot(3, 2, i+1)  \n",
    "    plt.subplots_adjust(wspace = 0.4, hspace = 0.6)\n",
    "    plt.title('message order = ' + str(i) )\n",
    "    plt.hist(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Now we repeat the same procedure but this time keeping the message_order constant and checking labels for different msgs\n",
    "\n",
    "for i in range(6):\n",
    "    x = labels[message_order == i]\n",
    "    plt.subplot(3, 2, i+1)  \n",
    "    plt.subplots_adjust(wspace = 0.4, hspace = 0.6)\n",
    "    plt.title('message order = ' + str(i) )\n",
    "    plt.hist(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There doesn't seem to be any plainly relevant relation/feature to be seen here so we shall proceed by implementing a model\n",
    "#as per the usual plan without any special crafted features. Let us begin by preparing the data for numeric conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shifting all data to lowercase and breaking sentences into word tokens\n",
    "\n",
    "def cleaning(message):\n",
    "    words = []\n",
    "    for s in message:\n",
    "        clean = re.sub(r'[^ a-z A-Z 0-9]', \" \", s)\n",
    "        w = word_tokenize(clean)\n",
    "        words.append([i.lower() for i in w])\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['sir'], ['k'], ['on', 'thanks'], ['hii'], ['sir', 'i', 'dnt', 'have', 'two', 'wheeler']]\n"
     ]
    }
   ],
   "source": [
    "cleaned_words = cleaning(message)\n",
    "print(cleaned_words[5:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard keras tokenizer procedure\n",
    "\n",
    "def create_tokenizer(words, filters = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~'):\n",
    "  token = Tokenizer(filters = filters)\n",
    "  token.fit_on_texts(words)\n",
    "  return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size = 812 and Maximum length = 19\n"
     ]
    }
   ],
   "source": [
    "word_tokenizer = create_tokenizer(cleaned_words)\n",
    "vocab_size = len(word_tokenizer.word_index) + 1\n",
    "max_length = len(max(cleaned_words, key = len))\n",
    "\n",
    "print(\"Vocab Size = %d and Maximum length = %d\" % (vocab_size, max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[16, 22], [11], [1], [1]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#obtaining the word vectors\n",
    "\n",
    "vectors = word_tokenizer.texts_to_sequences(cleaned_words)\n",
    "vectors[0:4]\n",
    "\n",
    "#using matrix decomposition of co-occurence matrix won't work well here clearly due to the nature of the 'corpus'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding zero padding to make size of all vectors equal\n",
    "\n",
    "def padding_doc(vectors, max_length):\n",
    "  return(pad_sequences(vectors, maxlen = max_length, padding = \"post\"))\n",
    "\n",
    "padded_doc = padding_doc(vectors, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 20)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Adding message order as an input feature\n",
    "\n",
    "message_order = message_order.reshape(2000,1)\n",
    "\n",
    "padded_doc = np.append(padded_doc, message_order, axis=1)\n",
    "padded_doc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2000, 5)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using one_hot representation for intent labels\n",
    "\n",
    "labels = labels.reshape(2000,1)\n",
    "labels.shape\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "Y_onehot = encoder.fit_transform(labels)\n",
    "Y_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, val_X, train_Y, val_Y = train_test_split(padded_doc, Y_onehot, shuffle = True, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_X = (1600, 20) and train_Y = (1600, 5)\n",
      "Shape of val_X = (400, 20) and val_Y = (400, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of train_X = %s and train_Y = %s\" % (train_X.shape, train_Y.shape))\n",
    "print(\"Shape of val_X = %s and val_Y = %s\" % (val_X.shape, val_Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#At this point we have our word vectors, output labels, training and validation datasets, and we can append the\n",
    "#message_order value as a feature to the word vectors and feed the data to a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#But our data is multilingual, to deal with that, we can first separate the data based on language and then train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using LSTM\n",
    "\n",
    "def create_model(vocab_size, max_length):\n",
    "  model = Sequential()\n",
    "  model.add(Embedding(vocab_size, 128, input_length = max_length, trainable = False))\n",
    "  model.add(Bidirectional(LSTM(128)))\n",
    "#   model.add(LSTM(128))\n",
    "  model.add(Dense(32, activation = \"relu\"))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(5, activation = \"softmax\"))\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 20, 128)           103936    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 256)               263168    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 375,493\n",
      "Trainable params: 271,557\n",
      "Non-trainable params: 103,936\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(vocab_size, max_length)\n",
    "\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 4s 2ms/step - loss: 1.5154 - acc: 0.2888 - val_loss: 1.4309 - val_acc: 0.3000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.43092, saving model to model.h5\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 1.4792 - acc: 0.3050 - val_loss: 1.4397 - val_acc: 0.3000\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.43092\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 1.4744 - acc: 0.3075 - val_loss: 1.4367 - val_acc: 0.3375\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.43092\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 1.4643 - acc: 0.3319 - val_loss: 1.4278 - val_acc: 0.3075\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.43092 to 1.42776, saving model to model.h5\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 1.4635 - acc: 0.3162 - val_loss: 1.4234 - val_acc: 0.3375\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.42776 to 1.42338, saving model to model.h5\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 2s 2ms/step - loss: 1.4573 - acc: 0.3081 - val_loss: 1.4321 - val_acc: 0.3000\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.42338\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.4587 - acc: 0.3181 - val_loss: 1.4212 - val_acc: 0.3225\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.42338 to 1.42125, saving model to model.h5\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 2s 2ms/step - loss: 1.4549 - acc: 0.3162 - val_loss: 1.4218 - val_acc: 0.3200\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.42125\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.4549 - acc: 0.3044 - val_loss: 1.4210 - val_acc: 0.3425\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.42125 to 1.42099, saving model to model.h5\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.4524 - acc: 0.3175 - val_loss: 1.4222 - val_acc: 0.3025\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.42099\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 1.4547 - acc: 0.2988 - val_loss: 1.4282 - val_acc: 0.3050\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.42099\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 1.4490 - acc: 0.3156 - val_loss: 1.4272 - val_acc: 0.3125\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.42099\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 1.4493 - acc: 0.3137 - val_loss: 1.4252 - val_acc: 0.3325\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.42099\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 1.4514 - acc: 0.3175 - val_loss: 1.4228 - val_acc: 0.3325\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.42099\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 1.4563 - acc: 0.3112 - val_loss: 1.4222 - val_acc: 0.3400\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.42099\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 1.4503 - acc: 0.3137 - val_loss: 1.4313 - val_acc: 0.3300\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.42099\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 1.4461 - acc: 0.3344 - val_loss: 1.4253 - val_acc: 0.3225\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.42099\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 1.4414 - acc: 0.3231 - val_loss: 1.4239 - val_acc: 0.3375\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.42099\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 1.4428 - acc: 0.3119 - val_loss: 1.4322 - val_acc: 0.3225\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.42099\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 1.4398 - acc: 0.3412 - val_loss: 1.4244 - val_acc: 0.3275\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.42099\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 1.4399 - acc: 0.3525 - val_loss: 1.4256 - val_acc: 0.3425\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.42099\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 1.4359 - acc: 0.3412 - val_loss: 1.4245 - val_acc: 0.3250\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.42099\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.4271 - acc: 0.3494 - val_loss: 1.4282 - val_acc: 0.3500\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.42099\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.4338 - acc: 0.3481 - val_loss: 1.4259 - val_acc: 0.3425\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.42099\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.4201 - acc: 0.3481 - val_loss: 1.4218 - val_acc: 0.3425\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.42099\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.4212 - acc: 0.3600 - val_loss: 1.4481 - val_acc: 0.3100\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.42099\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 1.4226 - acc: 0.3650 - val_loss: 1.4276 - val_acc: 0.3275\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.42099\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.4190 - acc: 0.3631 - val_loss: 1.4285 - val_acc: 0.3350\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.42099\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.4117 - acc: 0.3594 - val_loss: 1.4351 - val_acc: 0.3475\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.42099\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.4112 - acc: 0.3538 - val_loss: 1.4221 - val_acc: 0.3325\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.42099\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.3965 - acc: 0.3656 - val_loss: 1.4303 - val_acc: 0.3375\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.42099\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.3906 - acc: 0.3800 - val_loss: 1.4324 - val_acc: 0.3425\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.42099\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.3818 - acc: 0.3819 - val_loss: 1.4425 - val_acc: 0.3475\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.42099\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.3715 - acc: 0.3756 - val_loss: 1.4487 - val_acc: 0.3150\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.42099\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.3866 - acc: 0.3900 - val_loss: 1.4322 - val_acc: 0.3475\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.42099\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.3687 - acc: 0.3912 - val_loss: 1.4523 - val_acc: 0.3425\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 1.42099\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.3664 - acc: 0.3912 - val_loss: 1.4411 - val_acc: 0.3475\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.42099\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.3590 - acc: 0.4075 - val_loss: 1.4576 - val_acc: 0.3425\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 1.42099\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.3384 - acc: 0.4200 - val_loss: 1.4900 - val_acc: 0.3400\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.42099\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.3414 - acc: 0.4169 - val_loss: 1.4716 - val_acc: 0.3450\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 1.42099\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.3294 - acc: 0.4225 - val_loss: 1.4841 - val_acc: 0.3475\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 1.42099\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.3107 - acc: 0.4256 - val_loss: 1.5117 - val_acc: 0.3250\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 1.42099\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.3212 - acc: 0.4150 - val_loss: 1.4831 - val_acc: 0.3450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00043: val_loss did not improve from 1.42099\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 1.3072 - acc: 0.4100 - val_loss: 1.5295 - val_acc: 0.3350\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 1.42099\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 1.2943 - acc: 0.4250 - val_loss: 1.5325 - val_acc: 0.3425\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 1.42099\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.3125 - acc: 0.4288 - val_loss: 1.4918 - val_acc: 0.3625\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 1.42099\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.2879 - acc: 0.4469 - val_loss: 1.5765 - val_acc: 0.3250\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 1.42099\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 2s 2ms/step - loss: 1.2965 - acc: 0.4338 - val_loss: 1.5194 - val_acc: 0.3475\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 1.42099\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.2572 - acc: 0.4475 - val_loss: 1.5624 - val_acc: 0.3475\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 1.42099\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.2705 - acc: 0.4350 - val_loss: 1.5736 - val_acc: 0.3425\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 1.42099\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.2616 - acc: 0.4394 - val_loss: 1.5859 - val_acc: 0.3425\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 1.42099\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.2424 - acc: 0.4587 - val_loss: 1.6516 - val_acc: 0.3325\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 1.42099\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.2486 - acc: 0.4487 - val_loss: 1.5871 - val_acc: 0.3375\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 1.42099\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 1.2505 - acc: 0.4506 - val_loss: 1.5772 - val_acc: 0.3400\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 1.42099\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 1.2357 - acc: 0.4644 - val_loss: 1.6653 - val_acc: 0.3325\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 1.42099\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 1.2139 - acc: 0.4694 - val_loss: 1.6286 - val_acc: 0.3425\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 1.42099\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 2s 2ms/step - loss: 1.2132 - acc: 0.4694 - val_loss: 1.6502 - val_acc: 0.3450\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 1.42099\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 2s 2ms/step - loss: 1.2146 - acc: 0.4794 - val_loss: 1.7004 - val_acc: 0.3400\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 1.42099\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.2160 - acc: 0.4631 - val_loss: 1.6375 - val_acc: 0.3450\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 1.42099\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.2004 - acc: 0.4819 - val_loss: 1.7110 - val_acc: 0.3425\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 1.42099\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.1796 - acc: 0.4925 - val_loss: 1.7716 - val_acc: 0.3075\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 1.42099\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.1946 - acc: 0.4731 - val_loss: 1.7112 - val_acc: 0.3400\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 1.42099\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.1788 - acc: 0.4937 - val_loss: 1.8401 - val_acc: 0.3100\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 1.42099\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 2s 2ms/step - loss: 1.1902 - acc: 0.4725 - val_loss: 1.7209 - val_acc: 0.3125\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 1.42099\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 1.2007 - acc: 0.4763 - val_loss: 1.6286 - val_acc: 0.3450\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 1.42099\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 2s 2ms/step - loss: 1.1833 - acc: 0.4869 - val_loss: 1.7027 - val_acc: 0.3275\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 1.42099\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.1590 - acc: 0.4944 - val_loss: 1.7498 - val_acc: 0.3375\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 1.42099\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 2s 2ms/step - loss: 1.1623 - acc: 0.4856 - val_loss: 1.7478 - val_acc: 0.3400\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 1.42099\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.1627 - acc: 0.4919 - val_loss: 1.8202 - val_acc: 0.3275\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 1.42099\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.1379 - acc: 0.5000 - val_loss: 1.8476 - val_acc: 0.3200\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 1.42099\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 2s 2ms/step - loss: 1.1770 - acc: 0.4931 - val_loss: 1.7423 - val_acc: 0.3350\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 1.42099\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.1479 - acc: 0.5069 - val_loss: 1.7943 - val_acc: 0.3125\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 1.42099\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.2079 - acc: 0.4838 - val_loss: 1.7510 - val_acc: 0.3200\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 1.42099\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.1931 - acc: 0.4919 - val_loss: 1.6952 - val_acc: 0.3375\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 1.42099\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.1401 - acc: 0.5019 - val_loss: 1.7399 - val_acc: 0.2950\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 1.42099\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.1548 - acc: 0.4975 - val_loss: 1.7959 - val_acc: 0.3150\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 1.42099\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.1463 - acc: 0.4919 - val_loss: 1.8117 - val_acc: 0.3250\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 1.42099\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.1230 - acc: 0.5200 - val_loss: 1.8929 - val_acc: 0.3275\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 1.42099\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 2s 2ms/step - loss: 1.1214 - acc: 0.5231 - val_loss: 1.9552 - val_acc: 0.3150\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 1.42099\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.1227 - acc: 0.5200 - val_loss: 1.9249 - val_acc: 0.3125\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 1.42099\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.1112 - acc: 0.5069 - val_loss: 2.0430 - val_acc: 0.3275\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 1.42099\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.1981 - acc: 0.4956 - val_loss: 1.6691 - val_acc: 0.3150\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 1.42099\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.1741 - acc: 0.4994 - val_loss: 1.7247 - val_acc: 0.3350\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 1.42099\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.1409 - acc: 0.4956 - val_loss: 1.8037 - val_acc: 0.3275\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 1.42099\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.1378 - acc: 0.5031 - val_loss: 1.8509 - val_acc: 0.3375\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 1.42099\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.1169 - acc: 0.5200 - val_loss: 1.9323 - val_acc: 0.3325\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 1.42099\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 1.1010 - acc: 0.5206 - val_loss: 1.9148 - val_acc: 0.3375\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 1.42099\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 1.1057 - acc: 0.5169 - val_loss: 2.0024 - val_acc: 0.3275\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 1.42099\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 1.1175 - acc: 0.5119 - val_loss: 1.9371 - val_acc: 0.3100\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 1.42099\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 1.0864 - acc: 0.5219 - val_loss: 1.9816 - val_acc: 0.3125\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 1.42099\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 1.0965 - acc: 0.5238 - val_loss: 2.0525 - val_acc: 0.3150\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 1.42099\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 1.0775 - acc: 0.5350 - val_loss: 2.0926 - val_acc: 0.3300\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 1.42099\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 2s 2ms/step - loss: 1.1197 - acc: 0.5162 - val_loss: 1.8143 - val_acc: 0.2950\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 1.42099\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.1059 - acc: 0.5225 - val_loss: 1.8767 - val_acc: 0.3150\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 1.42099\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 1.0904 - acc: 0.5256 - val_loss: 1.9223 - val_acc: 0.3150\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 1.42099\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 1.0738 - acc: 0.5375 - val_loss: 2.0378 - val_acc: 0.3075\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 1.42099\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 1.0760 - acc: 0.5287 - val_loss: 2.0601 - val_acc: 0.3225\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 1.42099\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 1.0668 - acc: 0.5344 - val_loss: 2.0972 - val_acc: 0.3025\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 1.42099\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.0663 - acc: 0.5400 - val_loss: 2.1721 - val_acc: 0.3325\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 1.42099\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 2s 2ms/step - loss: 1.0852 - acc: 0.5306 - val_loss: 1.9558 - val_acc: 0.3175\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 1.42099\n"
     ]
    }
   ],
   "source": [
    "filename = 'model.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "hist = model.fit(train_X, train_Y, epochs = 100, batch_size = 32, validation_data = (val_X, val_Y), callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We see that even though the training loss is reducing, the validation loss is not and the model is only bit better\n",
    "#than a randomly classifying model.\n",
    "\n",
    "#There could be several reasons for that -\n",
    "\n",
    "#1. Understanding the importance of message_order and its weight in decision making for classification. \n",
    "\n",
    "#2. Need for further pre-processing of data, such as dealing with multiple languages so that vector representations\n",
    "#of the data are more accurate to semantics.\n",
    "\n",
    "#3. The word vectors are very sparse and perhaps need more processing to better represent the data.\n",
    "\n",
    "#4. There are 641 training examples with just the message 'hi' (or 'hello' or some variant). The curious thing \n",
    "#here is that for **entirely same inputs** i.e same (semantically) message 'hi' and same message_order, the assigned intents are all over\n",
    "#the place. Different outputs for same input will make it hard to classify properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2244389 , 0.26932668, 0.2575    , 0.27      , 0.26130653])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "cross_val_score(clf, padded_doc, labels, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We see a slightly worse performance using Decision Tree.\n",
    "\n",
    "#Although there is the difference between them that we have used only validation in one but cross-validation in the other.\n",
    "#In case of same performance, we would have preferred the one that has used cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
