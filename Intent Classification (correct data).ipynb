{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, Bidirectional, Embedding, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>label</th>\n",
       "      <th>new_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'id': 'KG0OUA', 'data': 'Good morning', 'message_order': 2, 'comments': ['']}</td>\n",
       "      <td>location</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'id': 'L9DC9H', 'data': 'Location', 'message_order': 5, 'comments': ['']}</td>\n",
       "      <td>whoAreYou</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'id': 'ZQR6R5', 'data': 'hi', 'message_order': 5, 'comments': ['']}</td>\n",
       "      <td>whoAreYou</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'id': 'RH0M4E', 'data': 'Hi', 'message_order': 4, 'comments': ['']}</td>\n",
       "      <td>greeting</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'id': 'WLVX8I', 'data': 'Hello', 'message_order': 1, 'comments': ['']}</td>\n",
       "      <td>greeting</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                             data  \\\n",
       "0  {'id': 'KG0OUA', 'data': 'Good morning', 'message_order': 2, 'comments': ['']}   \n",
       "1  {'id': 'L9DC9H', 'data': 'Location', 'message_order': 5, 'comments': ['']}       \n",
       "2  {'id': 'ZQR6R5', 'data': 'hi', 'message_order': 5, 'comments': ['']}             \n",
       "3  {'id': 'RH0M4E', 'data': 'Hi', 'message_order': 4, 'comments': ['']}             \n",
       "4  {'id': 'WLVX8I', 'data': 'Hello', 'message_order': 1, 'comments': ['']}          \n",
       "\n",
       "       label  new_label  \n",
       "0  location   3          \n",
       "1  whoAreYou  1          \n",
       "2  whoAreYou  3          \n",
       "3  greeting   3          \n",
       "4  greeting   3          "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('TakeHome_task_data.csv')\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>message_order</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'Good morning'</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'Location'</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'hi'</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'Hi'</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'Hello'</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              text  message_order  label\n",
       "0   'Good morning'  2              3    \n",
       "1   'Location'      5              1    \n",
       "2   'hi'            5              3    \n",
       "3   'Hi'            4              3    \n",
       "4   'Hello'         1              3    "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#obtaining the important parts of the data from the csv file i.e 'message', 'message_order', 'intent label'\n",
    "\n",
    "ndf = df['data'].str.split(',')\n",
    "\n",
    "ndata = list()\n",
    "for i in range(ndf.size):\n",
    "    ndata.append(ndf[i][1])\n",
    "    \n",
    "final_data = list()\n",
    "for i in range(len(ndata)):\n",
    "    final_data.append(ndata[i].split(':')[1])\n",
    "    \n",
    "m_order = list()\n",
    "for i in range(ndf.size):\n",
    "    if i in (75, 247, 458, 464, 631, 1269, 1459, 1601, 1651, 1803, 1918): #a bit crude but works for now\n",
    "        m_order.append(ndf[i][3])\n",
    "    else:\n",
    "        m_order.append(ndf[i][2])\n",
    "    \n",
    "final_m_order = list()\n",
    "for i in range(len(m_order)):\n",
    "    final_m_order.append(m_order[i].split(':')[1])\n",
    "\n",
    "    \n",
    "data = pd.DataFrame(final_data, columns = ['text']).assign(message_order = final_m_order)\n",
    "data = data.assign(label = df.label)\n",
    "data.text = data.text.apply(str)\n",
    "data.message_order = pd.to_numeric(data.message_order)\n",
    "data = data.assign(label = df.new_label)\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intent = data.label\n",
    "unique_intents = list(set(data.label))\n",
    "unique_intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_map = {'location': 1,'saidOK': 2, 'greeting' : 3, 'dontMeetRequirements' : 4, 'notInterested' : 5, 'hasLL' : 6}\n",
    "inv_map = {v: k for k, v in my_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting numeric labels to their corresponding text based label\n",
    "\n",
    "text_labels = list([inv_map[key] for key in data.label])\n",
    "data['text_label'] = text_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>message_order</th>\n",
       "      <th>label</th>\n",
       "      <th>text_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'Good morning'</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'Location'</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'hi'</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'Hi'</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'Hello'</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              text  message_order  label text_label\n",
       "0   'Good morning'  2              3      greeting \n",
       "1   'Location'      5              1      location \n",
       "2   'hi'            5              3      greeting \n",
       "3   'Hi'            4              3      greeting \n",
       "4   'Hello'         1              3      greeting "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualizing distribution of intent classification over message_orders (the dataset has fewer examples with class = 4,5)\n",
    "\n",
    "x = np.array(data.message_order)\n",
    "y = np.array(data.text_label)\n",
    "\n",
    "plt.xlabel('Message Order')\n",
    "plt.ylabel('Intent Class')\n",
    "\n",
    "plt.scatter(x,y, alpha = 0.007)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of class 1 = 233\n",
      "Examples of class 2 = 445\n",
      "Examples of class 3 = 828\n",
      "Examples of class 4 = 284\n",
      "Examples of class 5 = 112\n",
      "Examples of class 6 = 98\n"
     ]
    }
   ],
   "source": [
    "message = np.array(data.text)\n",
    "message_order = np.array(data.message_order)\n",
    "labels = np.array(data.label)\n",
    "\n",
    "for i in range(6):\n",
    "    print('Examples of class ' + str(i+1) + ' =', np.sum(labels == i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shifting all data to lowercase and breaking sentences into word tokens\n",
    "\n",
    "def cleaning(message):\n",
    "    words = []\n",
    "    for s in message:\n",
    "        clean = re.sub(r'[^ a-z A-Z 0-9]', \" \", s)\n",
    "        w = word_tokenize(clean)\n",
    "        words.append([i.lower() for i in w])\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['good', 'morning'], ['location'], ['hi'], ['hi'], ['hello'], ['sir'], ['k'], ['on', 'thanks'], ['hii'], ['sir', 'i', 'dnt', 'have', 'two', 'wheeler']]\n"
     ]
    }
   ],
   "source": [
    "cleaned_words = cleaning(message)\n",
    "print(cleaned_words[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 1, 1, 1, 1, 1, 2, 1, 6])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating feature word_count and checking its relation to the data using histogram\n",
    "\n",
    "word_count = []\n",
    "for i in range(len(cleaned_words)):\n",
    "    word_count.append(len(cleaned_words[i]))\n",
    "\n",
    "word_count = np.array(word_count)\n",
    "word_count[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 38.,   0., 410.,   0., 630.,   0.,  72.,   0.,  11.,  23.]),\n",
       " array([1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. , 5.5, 6. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAPTElEQVR4nO3dX4yddZ3H8fdHin8WxYIMpGmbrcbG1WwisBMWQ2Jcagz/YrmQrGYXGtJNb9Bg3MSt3hiTvcAbcUk2JA3VLbsoS1BCo0QlBeJ6ATpFBLEYuoSls0U6yh9F4hL0uxfzaxzaKXM6c84c5tf3K5k8z/N9fuc83+eCz/z4zXNOU1VIkvryhnE3IEkaPsNdkjpkuEtShwx3SeqQ4S5JHVo17gYAzjjjjNqwYcO425CkFWXv3r2/qqqJ+c69LsJ9w4YNTE1NjbsNSVpRkvzPsc65LCNJHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR16XXxCVVrIhu3fGct1n7zu0rFcV1oqZ+6S1CHDXZI6ZLhLUocMd0nqkOEuSR0aKNyTrE5ye5LHkuxL8oEkpye5O8njbXtaG5skNyTZn+ThJOeO9hYkSUcadOb+L8B3q+ovgPcD+4DtwJ6q2gjsaccAFwMb28824MahdixJWtCC4Z7kVOCDwE6Aqnq5qp4HNgO72rBdwOVtfzNwc826H1idZM3QO5ckHdMgM/d3ATPA15L8JMlNSU4BzqqqpwHa9sw2fi1wYM7rp1tNkrRMBgn3VcC5wI1VdQ7wO/60BDOfzFOrowYl25JMJZmamZkZqFlJ0mAGCfdpYLqqHmjHtzMb9s8cXm5p20Nzxq+f8/p1wMEj37SqdlTVZFVNTkzM+493S5IWacFwr6pfAgeSvKeVNgE/B3YDW1ptC3Bn298NXNWemjkfeOHw8o0kaXkM+sVhnwJuSfJG4AngamZ/MdyWZCvwFHBFG3sXcAmwH3ipjZUkLaOBwr2qHgIm5zm1aZ6xBVyzxL4kSUvgJ1QlqUOGuyR1yHCXpA4Z7pLUIcNdkjpkuEtShwx3SeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdGijckzyZ5JEkDyWZarXTk9yd5PG2Pa3Vk+SGJPuTPJzk3FHegCTpaMczc/+bqjq7qibb8XZgT1VtBPa0Y4CLgY3tZxtw47CalSQNZinLMpuBXW1/F3D5nPrNNet+YHWSNUu4jiTpOA0a7gV8P8neJNta7ayqehqgbc9s9bXAgTmvnW61V0myLclUkqmZmZnFdS9JmteqAcddUFUHk5wJ3J3ksdcYm3lqdVShagewA2BycvKo85KkxRto5l5VB9v2EHAHcB7wzOHllrY91IZPA+vnvHwdcHBYDUuSFrZguCc5JcnbDu8DHwF+BuwGtrRhW4A72/5u4Kr21Mz5wAuHl28kSctjkGWZs4A7khwe//Wq+m6SHwO3JdkKPAVc0cbfBVwC7AdeAq4eeteSpNe0YLhX1RPA++ep/xrYNE+9gGuG0p0kaVH8hKokdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA4Z7pLUIcNdkjpkuEtShwx3SeqQ4S5JHTLcJalDqwYdmOQkYAr436q6LMk7gVuB04EHgSur6uUkbwJuBv4K+DXwt1X15NA7P4Ft2P6dsV37yesuHdu1JQ3ueGbu1wL75hx/Cbi+qjYCzwFbW30r8FxVvRu4vo2TJC2jgcI9yTrgUuCmdhzgQuD2NmQXcHnb39yOaec3tfGSpGUy6Mz9K8BngT+243cAz1fVK+14Gljb9tcCBwDa+Rfa+FdJsi3JVJKpmZmZRbYvSZrPguGe5DLgUFXtnVueZ2gNcO5PhaodVTVZVZMTExMDNStJGswgf1C9APhokkuANwOnMjuTX51kVZudrwMOtvHTwHpgOskq4O3As0PvXJJ0TAvO3Kvqc1W1rqo2AB8H7qmqvwPuBT7Whm0B7mz7u9sx7fw9VXXUzF2SNDpLec79n4DPJNnP7Jr6zlbfCbyj1T8DbF9ai5Kk4zXwc+4AVXUfcF/bfwI4b54xvweuGEJvkqRF8hOqktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA4Z7pLUIcNdkjpkuEtShwx3SeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDi0Y7knenORHSX6a5NEkX2z1dyZ5IMnjSf4zyRtb/U3teH87v2G0tyBJOtIgM/f/Ay6sqvcDZwMXJTkf+BJwfVVtBJ4DtrbxW4HnqurdwPVtnCRpGS0Y7jXrxXZ4cvsp4ELg9lbfBVze9je3Y9r5TUkytI4lSQsaaM09yUlJHgIOAXcD/w08X1WvtCHTwNq2vxY4ANDOvwC8Y5733JZkKsnUzMzM0u5CkvQqA4V7Vf2hqs4G1gHnAe+db1jbzjdLr6MKVTuqarKqJicmJgbtV5I0gON6WqaqngfuA84HVidZ1U6tAw62/WlgPUA7/3bg2WE0K0kazCBPy0wkWd323wJ8GNgH3At8rA3bAtzZ9ne3Y9r5e6rqqJm7JGl0Vi08hDXAriQnMfvL4Laq+naSnwO3Jvln4CfAzjZ+J/DvSfYzO2P/+Aj6liS9hgXDvaoeBs6Zp/4Es+vvR9Z/D1wxlO4kSYviJ1QlqUOGuyR1yHCXpA4Z7pLUIcNdkjpkuEtShwx3SeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdWjDck6xPcm+SfUkeTXJtq5+e5O4kj7ftaa2eJDck2Z/k4STnjvomJEmvNsjM/RXgH6vqvcD5wDVJ3gdsB/ZU1UZgTzsGuBjY2H62ATcOvWtJ0mtaMNyr6umqerDt/xbYB6wFNgO72rBdwOVtfzNwc826H1idZM3QO5ckHdNxrbkn2QCcAzwAnFVVT8PsLwDgzDZsLXBgzsumW+3I99qWZCrJ1MzMzPF3Lkk6poHDPclbgW8Cn66q37zW0HlqdVShakdVTVbV5MTExKBtSJIGMFC4JzmZ2WC/paq+1crPHF5uadtDrT4NrJ/z8nXAweG0K0kaxCBPywTYCeyrqi/PObUb2NL2twB3zqlf1Z6aOR944fDyjSRpeawaYMwFwJXAI0kearXPA9cBtyXZCjwFXNHO3QVcAuwHXgKuHmrHkqQFLRjuVfVD5l9HB9g0z/gCrlliX5KkJfATqpLUIcNdkjpkuEtShwx3SeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA4tGO5JvprkUJKfzamdnuTuJI+37WmtniQ3JNmf5OEk546yeUnS/AaZuf8bcNERte3AnqraCOxpxwAXAxvbzzbgxuG0KUk6HguGe1X9AHj2iPJmYFfb3wVcPqd+c826H1idZM2wmpUkDWaxa+5nVdXTAG17ZquvBQ7MGTfdakdJsi3JVJKpmZmZRbYhSZrPsP+gmnlqNd/AqtpRVZNVNTkxMTHkNiTpxLbYcH/m8HJL2x5q9Wlg/Zxx64CDi29PkrQYiw333cCWtr8FuHNO/ar21Mz5wAuHl28kSctn1UIDknwD+BBwRpJp4AvAdcBtSbYCTwFXtOF3AZcA+4GXgKtH0LMkaQELhntVfeIYpzbNM7aAa5balCRpafyEqiR1yHCXpA4Z7pLUoQXX3CWNx4bt3xnLdZ+87tKxXFfD5cxdkjpkuEtShwx3SeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUodW/NcPjOsj2uDHtCW9fjlzl6QOGe6S1CHDXZI6ZLhLUodW/B9UJWmpenwww5m7JHXIcJekDhnuktQh19wlvW6Mc+27NyOZuSe5KMkvkuxPsn0U15AkHdvQwz3JScC/AhcD7wM+keR9w76OJOnYRjFzPw/YX1VPVNXLwK3A5hFcR5J0DKmq4b5h8jHgoqr6h3Z8JfDXVfXJI8ZtA7a1w/cAv1jkJc8AfrXI165U3vOJwXs+MSzlnv+8qibmOzGKP6hmntpRv0GqagewY8kXS6aqanKp77OSeM8nBu/5xDCqex7Fssw0sH7O8Trg4AiuI0k6hlGE+4+BjUnemeSNwMeB3SO4jiTpGIa+LFNVryT5JPA94CTgq1X16LCvM8eSl3ZWIO/5xOA9nxhGcs9D/4OqJGn8/PoBSeqQ4S5JHVqx4Z7kq0kOJfnZuHtZLknWJ7k3yb4kjya5dtw9jVqSNyf5UZKftnv+4rh7Wg5JTkrykyTfHncvyyHJk0keSfJQkqlx97MckqxOcnuSx9p/0x8Y6vuv1DX3JB8EXgRurqq/HHc/yyHJGmBNVT2Y5G3AXuDyqvr5mFsbmSQBTqmqF5OcDPwQuLaq7h9zayOV5DPAJHBqVV027n5GLcmTwGRVnTAfYEqyC/ivqrqpPVn4Z1X1/LDef8XO3KvqB8Cz4+5jOVXV01X1YNv/LbAPWDverkarZr3YDk9uPytzRjKgJOuAS4Gbxt2LRiPJqcAHgZ0AVfXyMIMdVnC4n+iSbADOAR4Ybyej15YoHgIOAXdXVe/3/BXgs8Afx93IMirg+0n2tq8m6d27gBnga2357aYkpwzzAob7CpTkrcA3gU9X1W/G3c+oVdUfqupsZj/tfF6SbpfhklwGHKqqvePuZZldUFXnMvttste0ZdeerQLOBW6sqnOA3wFD/Xp0w32FaevO3wRuqapvjbuf5dT+t/U+4KIxtzJKFwAfbWvQtwIXJvmP8bY0elV1sG0PAXcw++2yPZsGpuf8X+jtzIb90BjuK0j74+JOYF9VfXnc/SyHJBNJVrf9twAfBh4bb1ejU1Wfq6p1VbWB2a/uuKeq/n7MbY1UklPaAwK0pYmPAF0/BVdVvwQOJHlPK20ChvpgxIr9Z/aSfAP4EHBGkmngC1W1c7xdjdwFwJXAI20NGuDzVXXXGHsatTXArvaPwLwBuK2qTojHA08gZwF3zM5dWAV8vaq+O96WlsWngFvakzJPAFcP881X7KOQkqRjc1lGkjpkuEtShwx3SeqQ4S5JHTLcJalDhrskdchwl6QO/T/jlu5YOB+LEwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mask = (word_count <= 1)\n",
    "x = labels[mask] \n",
    "plt.hist(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\" 'बाइक नहीं है'\", \" 'मेरे पास बाइक नही है'\", \" 'లేదు'\",\n",
       "       \" 'जय महाराष्ट्र'\", \" 'లేదు'\", \" 'శుభోదయం'\",\n",
       "       \" '👾 ಬೆಳಗಿನ ಶುಭೋದಯ ಗಳು'\", \" 'कुटे है'\", \" '\", \" 'नमस्कार'\",\n",
       "       \" '😉✌'\"], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Non- English messages have 0 word count because of how the cleaning_sentences function was defined\n",
    "\n",
    "message[np.where(word_count == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard keras tokenizer procedure\n",
    "\n",
    "def create_tokenizer(words, filters = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~'):\n",
    "  token = Tokenizer(filters = filters)\n",
    "  token.fit_on_texts(words)\n",
    "  return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size = 812 and Maximum length = 19\n"
     ]
    }
   ],
   "source": [
    "word_tokenizer = create_tokenizer(cleaned_words)\n",
    "vocab_size = len(word_tokenizer.word_index) + 1\n",
    "max_length = len(max(cleaned_words, key = len))\n",
    "\n",
    "print(\"Vocab Size = %d and Maximum length = %d\" % (vocab_size, max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[16, 22], [11], [1], [1]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#obtaining the word vectors\n",
    "\n",
    "vectors = word_tokenizer.texts_to_sequences(cleaned_words)\n",
    "vectors[0:4]\n",
    "\n",
    "#using matrix decomposition of co-occurence matrix won't work well here clearly due to the nature of the 'corpus'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding zero padding to make size of all vectors equal\n",
    "\n",
    "def padding_doc(vectors, max_length):\n",
    "  return(pad_sequences(vectors, maxlen = max_length, padding = \"post\"))\n",
    "\n",
    "padded_doc = padding_doc(vectors, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding message order as an input feature\n",
    "\n",
    "#message_order = message_order.reshape(2000,1)\n",
    "\n",
    "#padded_doc = np.append(padded_doc, message_order, axis=1)\n",
    "#padded_doc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 20), 20)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adding word count as a feature\n",
    "\n",
    "word_count = np.array(word_count).reshape(2000,1)\n",
    "padded_doc = np.append(padded_doc, word_count, axis=1)\n",
    "max_length = padded_doc.shape[1]\n",
    "\n",
    "padded_doc.shape, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2000, 6)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using one_hot representation for intent labels\n",
    "\n",
    "labels = labels.reshape(2000,1)\n",
    "labels.shape\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "Y_onehot = encoder.fit_transform(labels)\n",
    "Y_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, val_X, train_Y, val_Y = train_test_split(padded_doc, Y_onehot, shuffle = True, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_X = (1600, 20) and train_Y = (1600, 6)\n",
      "Shape of val_X = (400, 20) and val_Y = (400, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of train_X = %s and train_Y = %s\" % (train_X.shape, train_Y.shape))\n",
    "print(\"Shape of val_X = %s and val_Y = %s\" % (val_X.shape, val_Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#At this point we have our word vectors, output labels, training and validation datasets, and we can append the\n",
    "#message_order value as a feature to the word vectors and feed the data to a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#But our data is multilingual, to deal with that, we can first separate the data based on language and then train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using LSTM\n",
    "\n",
    "def create_model(vocab_size, max_length):\n",
    "  model = Sequential()\n",
    "  model.add(Embedding(vocab_size, 128, input_length = max_length, trainable = False))\n",
    "  model.add(Bidirectional(LSTM(128)))\n",
    "#   model.add(LSTM(128))\n",
    "  model.add(Dense(32, activation = \"relu\"))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(6, activation = \"softmax\"))\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0728 22:19:22.079233 14440 deprecation_wrapper.py:119] From C:\\Users\\asus\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0728 22:19:22.099357 14440 deprecation_wrapper.py:119] From C:\\Users\\asus\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0728 22:19:22.102242 14440 deprecation_wrapper.py:119] From C:\\Users\\asus\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0728 22:19:22.529126 14440 deprecation_wrapper.py:119] From C:\\Users\\asus\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0728 22:19:22.539071 14440 deprecation.py:506] From C:\\Users\\asus\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0728 22:19:22.575996 14440 deprecation_wrapper.py:119] From C:\\Users\\asus\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0728 22:19:22.612875 14440 deprecation_wrapper.py:119] From C:\\Users\\asus\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 128)           103936    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 256)               263168    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 375,526\n",
      "Trainable params: 271,590\n",
      "Non-trainable params: 103,936\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(vocab_size, max_length)\n",
    "\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0728 22:19:22.833321 14440 deprecation.py:323] From C:\\Users\\asus\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/50\n",
      "1600/1600 [==============================] - 5s 3ms/step - loss: 1.6599 - acc: 0.3419 - val_loss: 1.4810 - val_acc: 0.4275\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.48099, saving model to model.h5\n",
      "Epoch 2/50\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 1.4891 - acc: 0.4450 - val_loss: 1.3019 - val_acc: 0.5125\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.48099 to 1.30186, saving model to model.h5\n",
      "Epoch 3/50\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 1.3504 - acc: 0.5200 - val_loss: 1.1264 - val_acc: 0.6450\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.30186 to 1.12639, saving model to model.h5\n",
      "Epoch 4/50\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 1.2003 - acc: 0.5931 - val_loss: 0.9738 - val_acc: 0.6475\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.12639 to 0.97382, saving model to model.h5\n",
      "Epoch 5/50\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 1.0878 - acc: 0.6212 - val_loss: 0.8917 - val_acc: 0.7050\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.97382 to 0.89171, saving model to model.h5\n",
      "Epoch 6/50\n",
      "1600/1600 [==============================] - 2s 2ms/step - loss: 0.9843 - acc: 0.6656 - val_loss: 0.7935 - val_acc: 0.7225\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.89171 to 0.79347, saving model to model.h5\n",
      "Epoch 7/50\n",
      "1600/1600 [==============================] - 2s 2ms/step - loss: 0.9191 - acc: 0.6894 - val_loss: 0.7175 - val_acc: 0.7600\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.79347 to 0.71752, saving model to model.h5\n",
      "Epoch 8/50\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.8357 - acc: 0.7144 - val_loss: 0.6989 - val_acc: 0.7475\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.71752 to 0.69891, saving model to model.h5\n",
      "Epoch 9/50\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.7821 - acc: 0.7400 - val_loss: 0.6510 - val_acc: 0.7850\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.69891 to 0.65104, saving model to model.h5\n",
      "Epoch 10/50\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.7440 - acc: 0.7625 - val_loss: 0.5701 - val_acc: 0.8075\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.65104 to 0.57014, saving model to model.h5\n",
      "Epoch 11/50\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.7033 - acc: 0.7656 - val_loss: 0.5616 - val_acc: 0.8200\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.57014 to 0.56159, saving model to model.h5\n",
      "Epoch 12/50\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.6520 - acc: 0.7913 - val_loss: 0.5469 - val_acc: 0.8300\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.56159 to 0.54686, saving model to model.h5\n",
      "Epoch 13/50\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.6407 - acc: 0.7881 - val_loss: 0.4976 - val_acc: 0.8575\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.54686 to 0.49761, saving model to model.h5\n",
      "Epoch 14/50\n",
      "1600/1600 [==============================] - 2s 2ms/step - loss: 0.5829 - acc: 0.8100 - val_loss: 0.4887 - val_acc: 0.8475\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.49761 to 0.48868, saving model to model.h5\n",
      "Epoch 15/50\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.5891 - acc: 0.8162 - val_loss: 0.4607 - val_acc: 0.8550\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.48868 to 0.46068, saving model to model.h5\n",
      "Epoch 16/50\n",
      "1600/1600 [==============================] - 2s 2ms/step - loss: 0.5802 - acc: 0.8263 - val_loss: 0.4559 - val_acc: 0.8525\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.46068 to 0.45589, saving model to model.h5\n",
      "Epoch 17/50\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.5358 - acc: 0.8381 - val_loss: 0.4357 - val_acc: 0.8650\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.45589 to 0.43574, saving model to model.h5\n",
      "Epoch 18/50\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.4934 - acc: 0.8456 - val_loss: 0.4357 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.43574\n",
      "Epoch 19/50\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.4970 - acc: 0.8387 - val_loss: 0.4369 - val_acc: 0.8650\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.43574\n",
      "Epoch 20/50\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.4828 - acc: 0.8550 - val_loss: 0.4217 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.43574 to 0.42172, saving model to model.h5\n",
      "Epoch 21/50\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.4767 - acc: 0.8538 - val_loss: 0.4148 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.42172 to 0.41479, saving model to model.h5\n",
      "Epoch 22/50\n",
      "1600/1600 [==============================] - 2s 2ms/step - loss: 0.4375 - acc: 0.8656 - val_loss: 0.4679 - val_acc: 0.8650\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.41479\n",
      "Epoch 23/50\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.4707 - acc: 0.8556 - val_loss: 0.4227 - val_acc: 0.8525\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.41479\n",
      "Epoch 24/50\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.4194 - acc: 0.8694 - val_loss: 0.4565 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.41479\n",
      "Epoch 25/50\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.4216 - acc: 0.8800 - val_loss: 0.3800 - val_acc: 0.8725\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.41479 to 0.37997, saving model to model.h5\n",
      "Epoch 26/50\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.3940 - acc: 0.8856 - val_loss: 0.3764 - val_acc: 0.8700\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.37997 to 0.37639, saving model to model.h5\n",
      "Epoch 27/50\n",
      "1600/1600 [==============================] - 2s 2ms/step - loss: 0.3723 - acc: 0.8906 - val_loss: 0.3850 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.37639\n",
      "Epoch 28/50\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.3666 - acc: 0.8850 - val_loss: 0.3903 - val_acc: 0.8825\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.37639\n",
      "Epoch 29/50\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.3487 - acc: 0.8962 - val_loss: 0.3945 - val_acc: 0.8800\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.37639\n",
      "Epoch 30/50\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.3522 - acc: 0.8944 - val_loss: 0.3536 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.37639 to 0.35356, saving model to model.h5\n",
      "Epoch 31/50\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.3514 - acc: 0.9050 - val_loss: 0.3842 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.35356\n",
      "Epoch 32/50\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.3149 - acc: 0.9094 - val_loss: 0.3648 - val_acc: 0.8950\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.35356\n",
      "Epoch 33/50\n",
      "1600/1600 [==============================] - 2s 2ms/step - loss: 0.3158 - acc: 0.9094 - val_loss: 0.3652 - val_acc: 0.8725\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.35356\n",
      "Epoch 34/50\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.3128 - acc: 0.9206 - val_loss: 0.3713 - val_acc: 0.8775\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.35356\n",
      "Epoch 35/50\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.3124 - acc: 0.9119 - val_loss: 0.3431 - val_acc: 0.8925\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.35356 to 0.34310, saving model to model.h5\n",
      "Epoch 36/50\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.3142 - acc: 0.9113 - val_loss: 0.3900 - val_acc: 0.8800\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.34310\n",
      "Epoch 37/50\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.3010 - acc: 0.9106 - val_loss: 0.3648 - val_acc: 0.8900\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.34310\n",
      "Epoch 38/50\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.2895 - acc: 0.9163 - val_loss: 0.3466 - val_acc: 0.8950\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.34310\n",
      "Epoch 39/50\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.2872 - acc: 0.9213 - val_loss: 0.4544 - val_acc: 0.8825\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.34310\n",
      "Epoch 40/50\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.2642 - acc: 0.9263 - val_loss: 0.3649 - val_acc: 0.8950\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.34310\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.2850 - acc: 0.9106 - val_loss: 0.3466 - val_acc: 0.8950\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.34310\n",
      "Epoch 42/50\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.2607 - acc: 0.9281 - val_loss: 0.4143 - val_acc: 0.8775\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.34310\n",
      "Epoch 43/50\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.2511 - acc: 0.9325 - val_loss: 0.4222 - val_acc: 0.8775\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.34310\n",
      "Epoch 44/50\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.2221 - acc: 0.9369 - val_loss: 0.4409 - val_acc: 0.8775\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.34310\n",
      "Epoch 45/50\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.2454 - acc: 0.9375 - val_loss: 0.3578 - val_acc: 0.8900\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.34310\n",
      "Epoch 46/50\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.2387 - acc: 0.9369 - val_loss: 0.3760 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.34310\n",
      "Epoch 47/50\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.2077 - acc: 0.9506 - val_loss: 0.3997 - val_acc: 0.8825\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.34310\n",
      "Epoch 48/50\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.2196 - acc: 0.9412 - val_loss: 0.3776 - val_acc: 0.8950\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.34310\n",
      "Epoch 49/50\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.2315 - acc: 0.9425 - val_loss: 0.3882 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.34310\n",
      "Epoch 50/50\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.2528 - acc: 0.9306 - val_loss: 0.3731 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.34310\n"
     ]
    }
   ],
   "source": [
    "filename = 'model.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "hist = model.fit(train_X, train_Y, epochs = 50, batch_size = 32, validation_data = (val_X, val_Y), callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We got a validation accuracy 0f ~88%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hasLL',\n",
       " 'location',\n",
       " 'location',\n",
       " 'greeting',\n",
       " 'saidOK',\n",
       " 'notInterested',\n",
       " 'dontMeetRequirements',\n",
       " 'location',\n",
       " 'greeting',\n",
       " 'location']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Making some predictions\n",
    "\n",
    "X = [\"I have ll not dl\", 'hy! were your office', 'tell me address', 'Hiiii!', 'oh ohk', 'stop messaging me', \n",
    "     'i dont have bike', 'good morning where is job', 'good morning tell address', \n",
    "     'good morning tell address of job']\n",
    "\n",
    "pred = cleaning(X)\n",
    "vector = word_tokenizer.texts_to_sequences(pred)\n",
    "padded = padding_doc(vector, max_length =19)\n",
    "\n",
    "count = []\n",
    "for i in range(len(pred)):\n",
    "    count.append(len(pred[i]))\n",
    "    \n",
    "count = np.array(count).reshape(len(count),1)\n",
    "padded = np.append(padded, count , axis=1)\n",
    "\n",
    "intent = [inv_map[model.predict_classes(padded)[i] + 1] for i in range(len(X))]\n",
    "intent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
